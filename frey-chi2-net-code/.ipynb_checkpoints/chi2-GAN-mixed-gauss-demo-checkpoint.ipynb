{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import prettytensor as pt\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "\n",
    "import scipy\n",
    "\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.flags\n",
    "logging = tf.logging\n",
    "\n",
    "flags.DEFINE_integer(\"batch_size\", 128, \"batch size\")\n",
    "flags.DEFINE_integer(\"updates_per_epoch\", 1000, \"number of updates per epoch\")\n",
    "flags.DEFINE_integer(\"max_epoch\", 100, \"max epoch\")\n",
    "flags.DEFINE_float(\"learning_rate\", 1e-2, \"learning rate\")\n",
    "flags.DEFINE_string(\"working_directory\", \"\", \"\")\n",
    "flags.DEFINE_integer(\"hidden_size\", 128, \"size of the hidden VAE unit\")\n",
    "# flags.DEFINE_integer(\"hidden_size\", 2, \"size of the hidden VAE unit\")\n",
    "\n",
    "flags.DEFINE_integer(\"dim\", 1, \"dimensionality of the target distribution\")\n",
    "flags.DEFINE_integer(\"gener_dim\", 20, \"dimensionality of the generator distribution\")\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(input_tensor):\n",
    "    '''Create discriminator network.\n",
    "\n",
    "    Args:\n",
    "        input_tensor: a batch of flattened images [batch_size, 28*28]\n",
    "\n",
    "    Returns:\n",
    "        A tensor that expresses the logit of being a true sample\n",
    "    '''\n",
    "\n",
    "    return (pt.wrap(input_tensor).\n",
    "            fully_connected(128).\n",
    "            fully_connected(128).\n",
    "            fully_connected(128).\n",
    "            dropout(0.9).\n",
    "            fully_connected(1, activation_fn=None)).tensor\n",
    "\n",
    "def generator(Z=None):\n",
    "    '''Create a generator network\n",
    "    \n",
    "    '''\n",
    "    if Z==None:\n",
    "        Z = tf.random_uniform([FLAGS.batch_size,FLAGS.gener_dim])\n",
    "    \n",
    "    return (pt.wrap(Z).\n",
    "            fully_connected(128).\n",
    "            fully_connected(128).\n",
    "            fully_connected(128).\n",
    "            fully_connected(FLAGS.dim, activation_fn=None)).tensor\n",
    "\n",
    "def chi2_loss(Xn,Yn):\n",
    "    C = tf.matmul(Xn,Yn,transpose_a=True)/FLAGS.batch_size # bias correction needed???\n",
    "    chi2 = tf.reduce_sum(tf.square(C))\n",
    "    return chi2\n",
    "\n",
    "# def get_gan_loss(input_tensor,generated_tensor):\n",
    "    \n",
    "#     with tf.variable_scope(\"model-discriminator\", reuse=True) as scope:\n",
    "#         D_input = discriminator(input_tensor)\n",
    "#     with tf.variable_scope(\"model-discriminator\", reuse=True) as scope:\n",
    "#         D_generated = discriminator(generated_tensor)\n",
    "        \n",
    "#     return tf.reduce_mean(D_input-tf.nn.softplus(D_input)-tf.nn.softplus(D_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtn_weights = [.3,.3,.3]\n",
    "mu = np.array([-1,1.5,4])\n",
    "sig = np.array([.5,.5,.5])\n",
    "\n",
    "\n",
    "def mg_sampler(n,mtn_weights,mu,sig):\n",
    "    mtn_samples = np.random.multinomial(1,mtn_weights,(n))\n",
    "    sampl_idx = np.argmax(mtn_samples,1)\n",
    "    mg_samples = np.reshape(mu[sampl_idx],[n,1])+np.reshape(sig[sampl_idx],[n,1])*np.random.randn(n,1)\n",
    "    return mg_samples\n",
    "\n",
    "# aa = mg_sampler(100000, mtn_weights,mu,sig)\n",
    "# plt.hist(aa,bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = tf.placeholder(tf.float32, [FLAGS.batch_size, FLAGS.dim])\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "# input_code = tf.placeholder(tf.float32, [FLAGS.batch_size, FLAGS.hidden_size])\n",
    "\n",
    "# ema = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "ema = tf.train.ExponentialMovingAverage(decay=0.8)\n",
    "\n",
    "label = tf.constant(np.concatenate((np.ones([FLAGS.batch_size,1]),\\\n",
    "                    -np.ones([FLAGS.batch_size,1]))),dtype=tf.float32)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "\n",
    "    with pt.defaults_scope(activation_fn=tf.nn.elu,\n",
    "                           batch_normalize=True,\n",
    "                           learned_moments_update_rate=0.0003,\n",
    "                           variance_epsilon=0.001,\n",
    "                           scale_after_normalization=True):\n",
    "        with pt.defaults_scope(phase=pt.Phase.train):\n",
    "            with tf.variable_scope(\"discriminator\") as scope:\n",
    "                D_input = discriminator(input_tensor)\n",
    "            with tf.variable_scope(\"generator\") as scope:\n",
    "                generated_tensor = generator()\n",
    "            with tf.variable_scope(\"discriminator\", reuse=True) as scope:\n",
    "                D_generated = discriminator(generated_tensor)\n",
    "\n",
    "        with pt.defaults_scope(phase=pt.Phase.test):\n",
    "            with tf.variable_scope(\"discriminator\", reuse=True) as scope:\n",
    "                D_input_test = discriminator(input_tensor)\n",
    "            with tf.variable_scope(\"generator\", reuse=True) as scope:\n",
    "                generated_tensor_test = generator()\n",
    "            with tf.variable_scope(\"discriminator\", reuse=True) as scope:\n",
    "                D_generated_test = discriminator(generated_tensor)\n",
    "                \n",
    "    D = tf.sigmoid(tf.concat((D_input,D_generated),axis=0))\n",
    "\n",
    "    D_mean,D_var = tf.nn.moments(D,axes=[0])\n",
    "    maintain_averages_op = ema.apply([D_mean, D_var])\n",
    "\n",
    "    ema_D_mean = ema.average(D_mean)\n",
    "    ema_D_std = tf.sqrt(ema.average(D_var))\n",
    "\n",
    "    Dn = (D-ema_D_mean)/ema_D_std\n",
    "\n",
    "    data_loss = tf.reduce_mean(D_input-tf.nn.softplus(D_input))\n",
    "    sampl_loss = -tf.reduce_mean(tf.nn.softplus(D_generated))\n",
    "    \n",
    "    discr_loss = -(data_loss+sampl_loss)\n",
    "    gener_loss = chi2_loss(Dn,label)\n",
    "    \n",
    "    discr_vars = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) if v.name.startswith('discriminator')]\n",
    "    gener_vars = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) if v.name.startswith('generator')]\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, epsilon=1.0)\n",
    "    \n",
    "    train_gan_discr = pt.apply_optimizer(optimizer, losses=[discr_loss], var_list=discr_vars)\n",
    "    train_gan_gener = pt.apply_optimizer(optimizer, losses=[gener_loss], var_list=gener_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1.475317530155182, 5.082407474517822]\n",
      "[2, 0.76314093846803366, 4.86935830116272]\n",
      "[3, 0.17332598644457289, 5.690716743469238]\n",
      "[4, 0.31082712606526913, 4.892812967300415]\n",
      "[5, 0.43151562875509264, 5.5575385093688965]\n",
      "[6, 0.54786040797829627, 4.824517011642456]\n",
      "[7, 0.57868378311395641, 4.692577123641968]\n",
      "[8, 0.58623844015598292, 4.533369779586792]\n",
      "[9, 0.59274677070975301, 4.517522573471069]\n",
      "[10, 0.58055584787577386, 4.52625846862793]\n",
      "[11, 0.56197652968019252, 4.517590761184692]\n",
      "[12, 0.54638612679392096, 4.955818176269531]\n",
      "[13, 0.52807139160484073, 4.885118007659912]\n",
      "[14, 0.48705342867970469, 4.495056629180908]\n",
      "[15, 0.42892089822143314, 4.50938868522644]\n",
      "[16, 0.37026572960242626, 4.492819547653198]\n",
      "[17, 0.31926256710290907, 4.5141284465789795]\n",
      "[18, 0.30337377323769033, 4.498210191726685]\n",
      "[19, 0.29702285630628467, 4.500646591186523]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MLOOP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-59cc985c50dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch_id\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mgener_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMLOOP\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmloop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLOOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MLOOP' is not defined"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "max_epoch = 100\n",
    "updates_per_epoch = 1000\n",
    "# max_epoch = 10\n",
    "# updates_per_epoch = 100\n",
    "# max_epoch = 100\n",
    "# updates_per_epoch = 100\n",
    "\n",
    "epoch_record = np.zeros([max_epoch,])\n",
    "gener_record = list()\n",
    "\n",
    "for epoch_id in range(max_epoch):\n",
    "    \n",
    "    loss_record = np.zeros([updates_per_epoch,])\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    for step in range(updates_per_epoch):\n",
    "\n",
    "#         x = np.random.randn(FLAGS.batch_size,1)\n",
    "        x = mg_sampler(FLAGS.batch_size,mtn_weights,mu,sig)\n",
    "        \n",
    "#         _,loss_val = sess.run([train_gan_discr,discr_loss],{input_tensor: x,\n",
    "#                     learning_rate: lr})\n",
    "        \n",
    "#         _,_ = sess.run([train_gan_gener,gener_loss],{input_tensor: x,\n",
    "#                     learning_rate: lr})\n",
    "        \n",
    "        _,loss_val,_ = sess.run([train_gan_discr,discr_loss,maintain_averages_op],\n",
    "                    {input_tensor: x, learning_rate: 100.*lr})\n",
    "        \n",
    "        _,loss_val,_ = sess.run([train_gan_gener,gener_loss,maintain_averages_op], \n",
    "                    {input_tensor: x, learning_rate: lr})\n",
    "        \n",
    "        loss_record[step] = loss_val\n",
    "        \n",
    "    if epoch_id>0 and np.mod(epoch_id+1,20)==0:\n",
    "        gener_samples = np.zeros([MLOOP*FLAGS.batch_size,])\n",
    "        for mloop in range(MLOOP):\n",
    "            y = sess.run(generated_tensor)\n",
    "            gener_samples[mloop*FLAGS.batch_size:(mloop+1)*FLAGS.batch_size] = np.reshape(y,[FLAGS.batch_size,])\n",
    "        plt.subplot(2,3,epoch_id/20+2)\n",
    "        _,_,_ = plt.hist((gener_samples,real_samples),bins=20)\n",
    "        _ = plt.legend({'Epoch %d' % (epoch_id+1)})\n",
    "\n",
    "        gener_record.append(gener_samples)\n",
    "        \n",
    "        \n",
    "    t1 = time()\n",
    "        \n",
    "    print([epoch_id+1,np.mean(loss_record),t1-t0])\n",
    "    epoch_record[epoch_id] = np.mean(loss_record)\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "_ = plt.plot(epoch_record)\n",
    "_ = plt.plot(range(19,max_epoch,20),epoch_record[19::20],'sg',alpha=.3)\n",
    "# _ = plt.title('Loss')\n",
    "# _ = plt.xlabel('epoch')\n",
    "_ = plt.legend({'Loss'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gener_samples = np.zeros([MLOOP*FLAGS.batch_size,])\n",
    "for mloop in range(MLOOP):\n",
    "    y = sess.run(generated_tensor)\n",
    "    gener_samples[mloop*FLAGS.batch_size:(mloop+1)*FLAGS.batch_size] = np.reshape(y,[FLAGS.batch_size,])\n",
    "gener_record.append(gener_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 12\n",
    "height = 6\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "for epoch_id in range(5):\n",
    "    plt.subplot(2,3,epoch_id+2)\n",
    "    gener_samples = gener_record[epoch_id]\n",
    "    _,_,_ = plt.hist((gener_samples,real_samples),bins=20,normed=True)\n",
    "    _ = plt.legend({'Epoch %d' % ((epoch_id+1)*20)},loc='upperleft')\n",
    "    _ = plt.xlim([-4,4])\n",
    "plt.subplot(2,3,1)\n",
    "_ = plt.plot(epoch_record)\n",
    "_ = plt.plot(range(19,max_epoch,20),epoch_record[19::20],'sg',alpha=.3)\n",
    "# _ = plt.title('Loss')\n",
    "# _ = plt.xlabel('epoch')\n",
    "_ = plt.legend({'Loss'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
